import os
import sqlite3
import requests
import json
import logging
import time
import threading
from dataclasses import dataclass
from datetime import datetime
from typing import List, Dict, Optional, Set
from newspaper import Article, Config as NewspaperConfig
from tqdm import tqdm
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import contextmanager
from urllib.parse import urlparse
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# --- é…ç½®æ—¥å¿—ç³»ç»Ÿ (å·²ä¼˜åŒ–) ---
# å…³é”®: ç»Ÿä¸€ä½¿ç”¨ utf-8 ç¼–ç ï¼Œå½»åº•è§£å†³ä¸­æ–‡ä¹±ç é—®é¢˜
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    encoding='utf-8',
    handlers=[
        logging.FileHandler('news_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- 1. æ•°æ®æ¨¡å‹å®šä¹‰ ---
@dataclass
class NewsArticle:
    """æ–°é—»æ–‡ç« æ•°æ®æ¨¡å‹"""
    title: str
    url: str
    source: str
    publish_date: str
    author: str
    sub_category: str
    category: str
    summary: str
    keywords: str
    value_score: int
    value_reason: str
    created_at: Optional[datetime] = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

@dataclass
class SearchTask:
    """æœç´¢ä»»åŠ¡æ¨¡å‹"""
    query: str
    sub_category: str
    type: str

# --- 2. é…ç½®ç®¡ç† (å·²ä¼˜åŒ–) ---
class AppConfig:
    """åº”ç”¨é…ç½®ç®¡ç†ç±»"""
    def __init__(self):
        load_dotenv()
        self.GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
        self.SEARCH_ENGINE_ID = os.getenv("SEARCH_ENGINE_ID")
        self.DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

        if not all([self.GOOGLE_API_KEY, self.SEARCH_ENGINE_ID, self.DEEPSEEK_API_KEY]):
            raise ValueError("é”™è¯¯ï¼šå¿…é¡»åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® GOOGLE_API_KEY, SEARCH_ENGINE_ID, å’Œ DEEPSEEK_API_KEY")
            
        self.DB_FILE = 'news.db'
        self.MAX_WORKERS = 10
        self.REQUEST_TIMEOUT = 20
        self.RETRY_COUNT = 3
        self.RETRY_DELAY = 2
        
        # åˆ›å»ºä¸€ä¸ªå¸¦é‡è¯•é€»è¾‘çš„å…¨å±€Session (æ–°å¢)
        self.http_session = self._create_resilient_session()
        
        # Newspaper3k é…ç½®
        self.newspaper_config = self._setup_newspaper_config()
        
    def _create_resilient_session(self) -> requests.Session:
        """åˆ›å»ºä¸€ä¸ªèƒ½è‡ªåŠ¨é‡è¯•çš„requests Sessionå¯¹è±¡ï¼Œå¢å¼ºç½‘ç»œè¯·æ±‚çš„å¥å£®æ€§"""
        session = requests.Session()
        retry_strategy = Retry(
            total=self.RETRY_COUNT,
            status_forcelist=[429, 500, 502, 503, 504],  # å¯¹è¿™äº›æœåŠ¡å™¨é”™è¯¯çŠ¶æ€ç è¿›è¡Œé‡è¯•
            allowed_methods=["HEAD", "GET", "POST"],
            backoff_factor=1  # é‡è¯•é—´éš”æ—¶é—´å› å­
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        return session
        
    def _setup_newspaper_config(self):
        """é…ç½®Newspaper3kï¼Œå¢åŠ æ›´å®Œæ•´çš„æµè§ˆå™¨å¤´ä¿¡æ¯"""
        config = NewspaperConfig()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        config.request_timeout = self.REQUEST_TIMEOUT
        config.headers = {
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        }
        return config

# --- 3. æ•°æ®åº“ç®¡ç† ---
class DatabaseManager:
    """SQLiteæ•°æ®åº“ç®¡ç†ç±»"""
    def __init__(self, db_path: str):
        self.db_path = db_path
        self._lock = threading.Lock()
        self._init_database()

    def _init_database(self):
        with self._get_connection() as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS articles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT, title TEXT NOT NULL, url TEXT UNIQUE NOT NULL,
                    source TEXT, publish_date TEXT, author TEXT, sub_category TEXT, category TEXT,
                    summary TEXT, keywords TEXT, value_score INTEGER, value_reason TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_url ON articles(url)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_publish_date ON articles(publish_date)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_value_score ON articles(value_score)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_category ON articles(category)')
            conn.commit()
    
    @contextmanager
    def _get_connection(self):
        conn = sqlite3.connect(self.db_path, timeout=10)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
        finally:
            conn.close()
    
    def get_existing_urls(self) -> Set[str]:
        with self._get_connection() as conn:
            cursor = conn.execute('SELECT url FROM articles')
            return {row['url'] for row in cursor}
    
    def save_articles(self, articles: List[NewsArticle]) -> int:
        if not articles:
            return 0
        with self._lock:
            with self._get_connection() as conn:
                saved_count = 0
                for article in articles:
                    try:
                        conn.execute('''
                            INSERT INTO articles (
                                title, url, source, publish_date, author, sub_category, category,
                                summary, keywords, value_score, value_reason
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (
                            article.title, article.url, article.source, article.publish_date,
                            article.author, article.sub_category, article.category, article.summary,
                            article.keywords, article.value_score, article.value_reason
                        ))
                        saved_count += 1
                    except sqlite3.IntegrityError:
                        logger.warning(f"æ–‡ç« å·²å­˜åœ¨: {article.url}")
                conn.commit()
                return saved_count

# --- 4. æœç´¢ç­–ç•¥ç®¡ç† ---
class SearchStrategyManager:
    """æœç´¢ç­–ç•¥ç®¡ç†ç±»"""
    SEARCH_TARGETS = [
        {"category": "å¹é£æœº", "terms": ["é«˜é€Ÿå¹é£æœº", "hair dryer"]},
        {"category": "ç¾å®¹ä»ª", "terms": ["ç¾å®¹ä»ª", "beauty device", "RF beauty device", "microcurrent device"]},
        {"category": "æŒ‰æ‘©ä»ª", "terms": ["ç­‹è†œæª", "massage gun"]},
    ]
    SEARCH_MODIFIERS = [
        {"type": "æŠ€æœ¯åˆ›æ–°", "terms": ["å±•ä¼š","æŠ€æœ¯", "æ–°å“", "ä¸“åˆ©", "new technology", "innovation", "patent", "launch"]},
        {"type": "å¸‚åœºè¯„æµ‹", "terms": ["è¯„æµ‹", "å¯¹æ¯”", "review", "vs"]},
        {"type": "æ³•è§„ç›‘ç®¡", "terms": ["ç›‘ç®¡", "æ³•è§„", "FDA approval", "clinical trial"]},
        {"type": "è¡Œä¸šæŠ¥å‘Š", "terms": ["è¡Œä¸šæŠ¥å‘Š", "å¸‚åœºè¶‹åŠ¿", "market research", "industry report"]},
    ]
    TARGETED_SOURCES = [
        {"domain": "36kr.com", "keywords": ["ç¾å®¹ç§‘æŠ€", "ä¸ªæŠ¤å®¶ç”µ", "æ¶ˆè´¹ç”µå­"]},
        {"domain": "geekpark.net", "keywords": ["æ™ºèƒ½ç¡¬ä»¶", "æ¶ˆè´¹ç”µå­"]},
        {"domain": "techcrunch.com", "keywords": ["beauty tech", "personal care", "hardware"]},
        {"domain": "theverge.com", "keywords": ["personal care", "gadgets", "review"]},
    ]
    # æ‰©å±•é»‘åå•ï¼ŒåŠ å…¥æ—¥å¿—ä¸­é¢‘ç¹å¤±è´¥æˆ–æ— ä»·å€¼çš„åŸŸå (å·²ä¼˜åŒ–)
    DOMAIN_BLACKLIST = {
        # ç¤¾äº¤å’Œè§†é¢‘å¹³å°
        'youtube.com', 'bilibili.com', 'facebook.com', 'twitter.com', 'instagram.com',
        'pinterest.com', 'linkedin.com', 'reddit.com', 'tiktok.com', 'quora.com', 'zhihu.com', 'sohu.com', 'theverge.com',
        # ç”µå•†å’Œå›¾ç‰‡ç´ æç½‘ç«™
        'amazon.com', 'tmall.com', 'jd.com', 'taobao.com', 'aliexpress.com', 'vmall.com',
        'shutterstock.com', 'adobestock.com', 'gettyimages.com', 'behance.net',
        # æ—¥å¿—ä¸­æ˜ç¡®å‡ºç°é—®é¢˜ä¸”éæ ¸å¿ƒä¿¡æ¯æºçš„ç½‘ç«™
        'yohohongkong.com', 'bukshisha.com', 'drselimguldiken.com', 'winbo4x4.com',
        'tortillasdurum.com', 'centrooleodinamica.com', 'queenstreetonline.com',
        'shoplc.com', 'i5a6.com', 'revnabio.com', 'wizevo.com', 'huadicn.com',
        'manuals.plus', 'inspire.com', 'medestheticsmag.com', 'fittopfactory.com'
    }
    # æ–°å¢æ–‡ä»¶æ‰©å±•åé»‘åå• (å·²ä¼˜åŒ–)
    EXTENSION_BLACKLIST = {'.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.rar', '.docx', '.xlsx', '.mp4', '.avi'}

    @classmethod
    def generate_search_tasks(cls) -> List[SearchTask]:
        tasks = []
        for target in cls.SEARCH_TARGETS:
            for term in target["terms"]:
                for modifier in cls.SEARCH_MODIFIERS:
                    for mod_term in modifier["terms"]:
                        query = f'"{term}" "{mod_term}"'
                        tasks.append(SearchTask(query=query, sub_category=target["category"], type=modifier["type"]))
        for source in cls.TARGETED_SOURCES:
            for keyword in source["keywords"]:
                query = f'"{keyword}" site:{source["domain"]}'
                tasks.append(SearchTask(query=query, sub_category="è¡Œä¸šåŠ¨æ€", type=source["domain"]))
        logger.info(f"ç”Ÿæˆäº† {len(tasks)} ä¸ªæœç´¢ä»»åŠ¡")
        return tasks

    @classmethod
    def is_url_blacklisted(cls, url: str) -> bool:
        """æ£€æŸ¥URLçš„åŸŸåæˆ–æ–‡ä»¶æ‰©å±•åæ˜¯å¦åœ¨é»‘åå•ä¸­ (å·²ä¼˜åŒ–)"""
        if not url or not url.startswith(('http://', 'https://')):
            return True
        try:
            parsed_url = urlparse(url)
            domain = parsed_url.netloc
            path = parsed_url.path.lower()
            
            # æ£€æŸ¥åŸŸå
            if any(blacklisted_domain in domain for blacklisted_domain in cls.DOMAIN_BLACKLIST):
                logger.debug(f"URLåŸŸåè¢«å±è”½: {url}")
                return True
            
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            if any(path.endswith(ext) for ext in cls.EXTENSION_BLACKLIST):
                logger.debug(f"URLæ–‡ä»¶ç±»å‹è¢«å±è”½: {url}")
                return True
                
            return False
        except Exception as e:
            logger.warning(f"URLè§£æå¤±è´¥ '{url}', é”™è¯¯: {e}. å°†å…¶è§†ä¸ºé»‘åå•URL.")
            return True

# --- 5. APIå®¢æˆ·ç«¯ (å·²ä¼˜åŒ–) ---
class GoogleSearchClient:
    """Googleæœç´¢APIå®¢æˆ·ç«¯"""
    def __init__(self, api_key: str, search_engine_id: str, session: requests.Session):
        self.api_key = api_key
        self.search_engine_id = search_engine_id
        self.base_url = "https://www.googleapis.com/customsearch/v1"
        self.session = session  # ä½¿ç”¨ä¼ å…¥çš„å¸¦é‡è¯•çš„session

    def search(self, query: str, num: int = 5) -> List[Dict]:
        params = {'key': self.api_key, 'cx': self.search_engine_id, 'q': query, 'num': num}
        try:
            response = self.session.get(self.base_url, params=params, timeout=15)
            response.raise_for_status()
            return response.json().get('items', [])
        except requests.RequestException as e:
            logger.error(f"Googleæœç´¢å¤±è´¥: {e}")
            return []

class DeepSeekClient:
    """DeepSeek APIå®¢æˆ·ç«¯"""
    def __init__(self, api_key: str, session: requests.Session):
        self.api_key = api_key
        self.base_url = "https://api.deepseek.com/chat/completions"
        self.headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        self.session = session  # ä½¿ç”¨ä¼ å…¥çš„å¸¦é‡è¯•çš„session

    def _make_request(self, prompt: str, temperature: float = 0.1) -> Optional[str]:
        response = None  # ä¿®å¤: åœ¨ try å—ä¹‹å‰åˆå§‹åŒ– response
        try:
            response = self.session.post(
                self.base_url, headers=self.headers,
                json={"model": "deepseek-chat", "messages": [{"role": "user", "content": prompt}], "temperature": temperature, "stream": False},
                timeout=45
            )
            response.raise_for_status()
            return response.json()['choices'][0]['message']['content']
        except requests.RequestException as e:
            logger.error(f"DeepSeek APIè¯·æ±‚å¤±è´¥: {e}")
            return None
        except (KeyError, IndexError) as e:
            response_text = response.text if response else 'N/A'
            logger.error(f"DeepSeek APIå“åº”æ ¼å¼ä¸æ­£ç¡®: {e} - å“åº”: {response_text}")
            return None

    def _parse_json_response(self, content: Optional[str]) -> Optional[Dict]:
        """å¥å£®çš„JSONè§£æå‡½æ•° (å·²ä¼˜åŒ–)"""
        if not content:
            return None
        try:
            # ç§»é™¤å¸¸è§çš„Markdownä»£ç å—æ ‡è®°
            if content.strip().startswith("```json"):
                content = content.strip()[7:-3]
            return json.loads(content)
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}. åŸå§‹å†…å®¹: {content[:500]}...")
            return None

    def analyze_article(self, title: str, text: str) -> Optional[Dict]:
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡Œä¸šåˆ†æå¸ˆï¼Œä¸“æ³¨äºä¸ªæŠ¤å°å®¶ç”µé¢†åŸŸã€‚è¯·åˆ†æä»¥ä¸‹æ–‡ç« ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚

        æ–‡ç« æ ‡é¢˜: {title}
        æ–‡ç« å…¨æ–‡ï¼ˆéƒ¨åˆ†ï¼‰:
        {text[:3000]}

        è¯·æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
        1. **category**: åˆ¤æ–­æ–‡ç« çš„æ ¸å¿ƒç±»åˆ«ã€‚åªèƒ½ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªï¼š["æŠ€æœ¯åˆ›æ–°", "å¸‚åœºåŠ¨æ€", "æ³•è§„æ”¿ç­–", "ç«å“åˆ†æ", "ç”¨æˆ·åé¦ˆ", "è¡Œä¸šæŠ¥å‘Š", "æ— å…³"]ã€‚
        2. **summary**: ç”Ÿæˆä¸€æ®µä¸è¶…è¿‡200å­—çš„ä¸­æ–‡æ‘˜è¦ï¼Œç²¾å‡†æ¦‚æ‹¬æ–‡ç« æ ¸å¿ƒå†…å®¹ã€‚
        3. **keywords**: æå–3-5ä¸ªæœ€å…³é”®çš„ä¸­æ–‡å…³é”®è¯ã€‚

        è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
        {{
          "category": "...",
          "summary": "...",
          "keywords": ["...", "...", "..."]
        }}
        """
        content = self._make_request(prompt)
        return self._parse_json_response(content)

    def evaluate_value(self, summary: str, sub_category: str) -> Optional[Dict]:
        prompt = f"""
        ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ä¸ªæŠ¤å°å®¶ç”µäº§å“çº¿è´Ÿè´£äººã€‚è¯·åŸºäºä»¥ä¸‹æƒ…æŠ¥æ‘˜è¦ï¼Œè¯„ä¼°å…¶å¯¹äºæˆ‘ä»¬ä¸šåŠ¡çš„ä»·å€¼ã€‚

        æƒ…æŠ¥å­åˆ†ç±»: {sub_category}
        æƒ…æŠ¥æ‘˜è¦: {summary}

        è¯·æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
        1. **score**: å¯¹æƒ…æŠ¥çš„ä¸šåŠ¡ä»·å€¼æ‰“åˆ†ï¼ŒèŒƒå›´0-100ã€‚
        2. **reason**: ç”¨ä¸€å¥è¯ç®€æ˜æ‰¼è¦åœ°è§£é‡Šæ‰“åˆ†åŸå› ã€‚

        è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONï¼š
        {{
            "score": ...,
            "reason": "..."
        }}
        """
        content = self._make_request(prompt)
        return self._parse_json_response(content)

# --- 6. æ–‡ç« å¤„ç†å™¨ (å·²ä¼˜åŒ–) ---
class ArticleProcessor:
    """æ–‡ç« å¤„ç†å™¨"""
    def __init__(self, config: AppConfig, deepseek_client: DeepSeekClient):
        self.config = config
        self.deepseek_client = deepseek_client
    
    def fetch_article(self, url: str) -> Optional[Article]:
        """æŠ“å–æ–‡ç« å†…å®¹ï¼Œå¹¶å¢åŠ æ‰‹åŠ¨é‡è¯•é€»è¾‘ (å·²ä¼˜åŒ–)"""
        for attempt in range(self.config.RETRY_COUNT):
            try:
                article = Article(url, config=self.config.newspaper_config)
                article.download()
                article.parse()
                return article
            except Exception as e:
                logger.warning(f"æ–‡ç« æŠ“å–å¤±è´¥ (ç¬¬ {attempt + 1}/{self.config.RETRY_COUNT} æ¬¡): {url}, é”™è¯¯: {e}")
                if attempt < self.config.RETRY_COUNT - 1:
                    time.sleep(self.config.RETRY_DELAY)
        logger.error(f"æ–‡ç« æŠ“å–æœ€ç»ˆå¤±è´¥: {url}")
        return None

    def extract_metadata(self, item: Dict) -> tuple:
        pagemap = item.get('pagemap', {})
        metatags = pagemap.get('metatags', [{}])[0]
        publish_date = metatags.get('article:published_time', 'æœªçŸ¥')
        author = metatags.get('author', 'æœªçŸ¥')
        return publish_date, author
    
    def process_single_article(self, search_result: Dict, task: SearchTask) -> Optional[NewsArticle]:
        url = search_result.get('link')
        if not url or SearchStrategyManager.is_url_blacklisted(url):
            return None
        
        title = search_result.get('title', '')
        display_link = search_result.get('displayLink', '')
        publish_date, author = self.extract_metadata(search_result)
        
        article = self.fetch_article(url)
        if not article or not article.text or len(article.text) < 200:
            logger.debug(f"æ–‡ç« å†…å®¹è¿‡çŸ­æˆ–æ— æ³•è·å–: {url}")
            return None
        
        if publish_date and publish_date != "æœªçŸ¥":
            publish_date = publish_date.split('T')[0]
        # ä¿®å¤: æ£€æŸ¥ article.publish_date æ˜¯å¦ä¸º datetime å¯¹è±¡
        elif article.publish_date and isinstance(article.publish_date, datetime):
            publish_date = article.publish_date.strftime('%Y-%m-%d')
        else:
            publish_date = "æœªçŸ¥"
        
        analysis = self.deepseek_client.analyze_article(title, article.text)
        if not analysis or analysis.get('category') == 'æ— å…³':
            logger.debug(f"æ–‡ç« è¢«AIè¿‡æ»¤: {title[:50]}...")
            return None
        
        summary = analysis.get('summary', '')
        evaluation = self.deepseek_client.evaluate_value(summary, task.sub_category)
        value_score, value_reason = (evaluation.get('score', 0), evaluation.get('reason', 'è¯„ä¼°å¤±è´¥')) if evaluation else (0, 'è¯„ä¼°å¤±è´¥')
        
        return NewsArticle(
            title=title, url=url, source=display_link, publish_date=publish_date,
            author=author, sub_category=task.sub_category, category=analysis.get('category', 'å…¶ä»–'),
            summary=summary, keywords=", ".join(analysis.get('keywords', [])),
            value_score=value_score, value_reason=value_reason
        )

# --- 7. ä¸»ç¨‹åº (å·²ä¼˜åŒ–) ---
class NewsScraper:
    """æ–°é—»æŠ“å–ä¸»ç¨‹åº"""
    def __init__(self):
        self.config = AppConfig()
        self.db_manager = DatabaseManager(self.config.DB_FILE)
        # å°†åˆ›å»ºå¥½çš„å¸¦é‡è¯•åŠŸèƒ½çš„sessionæ³¨å…¥åˆ°å„ä¸ªå®¢æˆ·ç«¯
        # ä¿®å¤: æ·»åŠ æ–­è¨€ä»¥è§£å†³ç±»å‹æ£€æŸ¥å™¨çš„è­¦å‘Š
        assert self.config.GOOGLE_API_KEY is not None
        assert self.config.SEARCH_ENGINE_ID is not None
        assert self.config.DEEPSEEK_API_KEY is not None
        
        self.google_client = GoogleSearchClient(
            self.config.GOOGLE_API_KEY, self.config.SEARCH_ENGINE_ID, self.config.http_session
        )
        self.deepseek_client = DeepSeekClient(self.config.DEEPSEEK_API_KEY, self.config.http_session)
        self.article_processor = ArticleProcessor(self.config, self.deepseek_client)
        self.existing_urls = self.db_manager.get_existing_urls()
    
    def process_search_task(self, task: SearchTask) -> List[NewsArticle]:
        logger.info(f"æ‰§è¡Œæœç´¢: {task.query}")
        search_results = self.google_client.search(task.query, num=10)
        if not search_results:
            return []
        
        articles = []
        for result in search_results:
            url = result.get('link')
            if url and url not in self.existing_urls:
                self.existing_urls.add(url) # æå‰åŠ å…¥ï¼Œé¿å…å¹¶å‘ä»»åŠ¡é‡å¤å¤„ç†
                article = self.article_processor.process_single_article(result, task)
                if article:
                    articles.append(article)
                    logger.info(f"æˆåŠŸå¤„ç†: {article.title[:50]}... (ä»·å€¼åˆ†: {article.value_score})")
        return articles
    
    def run(self):
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæƒ…æŠ¥æŠ“å–ä»»åŠ¡...")
        logger.info(f"ğŸ“Š æ•°æ®åº“ä¸­å·²å­˜åœ¨ {len(self.existing_urls)} æ¡è®°å½•")
        
        search_tasks = SearchStrategyManager.generate_search_tasks()
        all_articles = []
        
        with ThreadPoolExecutor(max_workers=self.config.MAX_WORKERS) as executor:
            future_to_task = {executor.submit(self.process_search_task, task): task for task in search_tasks}
            
            progress_bar = tqdm(as_completed(future_to_task), total=len(search_tasks), desc="å¤„ç†æœç´¢ä»»åŠ¡")
            for future in progress_bar:
                try:
                    articles = future.result()
                    if articles:
                        all_articles.extend(articles)
                except Exception as e:
                    task = future_to_task[future]
                    logger.error(f"ä»»åŠ¡ '{task.query}' æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
                time.sleep(0.5) # è½»å¾®å»¶æ—¶ï¼Œé¿å…è¿‡äºé¢‘ç¹
        
        if all_articles:
            saved_count = self.db_manager.save_articles(all_articles)
            logger.info(f"âœ… æˆåŠŸä¿å­˜ {saved_count} ç¯‡æ–°æ–‡ç« ")
            
            high_value_articles = [a for a in all_articles if a.value_score >= 70]
            if high_value_articles:
                logger.info("\nğŸŒŸ é«˜ä»·å€¼æƒ…æŠ¥ (â‰¥70åˆ†):")
                for article in sorted(high_value_articles, key=lambda x: x.value_score, reverse=True):
                    logger.info(f"  [{article.value_score}åˆ†] {article.title[:60]}...")
                    logger.info(f"    ç†ç”±: {article.value_reason}")
        else:
            logger.info("æœ¬æ¬¡è¿è¡Œæœªå‘ç°æ–°æƒ…æŠ¥")

# --- 8. ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    scraper = NewsScraper()
    scraper.run()
