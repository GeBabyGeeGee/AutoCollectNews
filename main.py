import os
import sqlite3
import requests
import json
import logging
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import List, Dict, Optional, Set
from newspaper import Article, Config as NewspaperConfig
from tqdm import tqdm
from dotenv import load_dotenv
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import contextmanager
from urllib.parse import urlparse

# --- é…ç½®æ—¥å¿—ç³»ç»Ÿ ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('news_scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- 1. æ•°æ®æ¨¡å‹å®šä¹‰ ---
@dataclass
class NewsArticle:
    """æ–°é—»æ–‡ç« æ•°æ®æ¨¡å‹"""
    title: str
    url: str
    source: str
    publish_date: str
    author: str
    sub_category: str
    category: str
    summary: str
    keywords: str
    value_score: int
    value_reason: str
    created_at: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

@dataclass
class SearchTask:
    """æœç´¢ä»»åŠ¡æ¨¡å‹"""
    query: str
    sub_category: str
    type: str

# --- 2. é…ç½®ç®¡ç† ---
class AppConfig:
    """åº”ç”¨é…ç½®ç®¡ç†ç±»"""
    def __init__(self):
        load_dotenv()
        self.GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
        self.SEARCH_ENGINE_ID = os.getenv("SEARCH_ENGINE_ID")
        self.DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
        self.DB_FILE = 'news.db'
        self.MAX_WORKERS = 10
        self.REQUEST_TIMEOUT = 20
        self.RETRY_COUNT = 3
        self.RETRY_DELAY = 2
        
        # Newspaper3k é…ç½®
        self.newspaper_config = self._setup_newspaper_config()
        
    def _setup_newspaper_config(self):
        config = NewspaperConfig()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0'
        config.request_timeout = self.REQUEST_TIMEOUT
        return config

# --- 3. æ•°æ®åº“ç®¡ç† ---
class DatabaseManager:
    """SQLiteæ•°æ®åº“ç®¡ç†ç±»"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        with self._get_connection() as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS articles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    title TEXT NOT NULL,
                    url TEXT UNIQUE NOT NULL,
                    source TEXT,
                    publish_date TEXT,
                    author TEXT,
                    sub_category TEXT,
                    category TEXT,
                    summary TEXT,
                    keywords TEXT,
                    value_score INTEGER,
                    value_reason TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
            conn.execute('CREATE INDEX IF NOT EXISTS idx_url ON articles(url)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_publish_date ON articles(publish_date)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_value_score ON articles(value_score)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_category ON articles(category)')
            conn.commit()
    
    @contextmanager
    def _get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
        finally:
            conn.close()
    
    def get_existing_urls(self) -> Set[str]:
        """è·å–å·²å­˜åœ¨çš„URLé›†åˆ"""
        with self._get_connection() as conn:
            cursor = conn.execute('SELECT url FROM articles')
            return {row['url'] for row in cursor}
    
    def save_articles(self, articles: List[NewsArticle]) -> int:
        """æ‰¹é‡ä¿å­˜æ–‡ç« """
        if not articles:
            return 0
            
        with self._get_connection() as conn:
            saved_count = 0
            for article in articles:
                try:
                    conn.execute('''
                        INSERT INTO articles (
                            title, url, source, publish_date, author,
                            sub_category, category, summary, keywords,
                            value_score, value_reason
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        article.title, article.url, article.source,
                        article.publish_date, article.author,
                        article.sub_category, article.category,
                        article.summary, article.keywords,
                        article.value_score, article.value_reason
                    ))
                    saved_count += 1
                except sqlite3.IntegrityError:
                    logger.warning(f"æ–‡ç« å·²å­˜åœ¨: {article.url}")
            
            conn.commit()
            return saved_count
    
    def get_recent_articles(self, limit: int = 10) -> List[Dict]:
        """è·å–æœ€è¿‘çš„æ–‡ç« """
        with self._get_connection() as conn:
            cursor = conn.execute('''
                SELECT * FROM articles
                ORDER BY created_at DESC
                LIMIT ?
            ''', (limit,))
            return [dict(row) for row in cursor]
    
    def get_high_value_articles(self, min_score: int = 70) -> List[Dict]:
        """è·å–é«˜ä»·å€¼æ–‡ç« """
        with self._get_connection() as conn:
            cursor = conn.execute('''
                SELECT * FROM articles
                WHERE value_score >= ?
                ORDER BY value_score DESC, created_at DESC
            ''', (min_score,))
            return [dict(row) for row in cursor]

# --- 4. æœç´¢ç­–ç•¥ç®¡ç† ---
class SearchStrategyManager:
    """æœç´¢ç­–ç•¥ç®¡ç†ç±»"""
    
    SEARCH_TARGETS = [
        {"category": "å¹é£æœº", "terms": ["é«˜é€Ÿå¹é£æœº", "hair dryer"]},
        {"category": "ç¾å®¹ä»ª", "terms": ["ç¾å®¹ä»ª", "beauty device", "RF beauty device", "microcurrent device"]},
        {"category": "æŒ‰æ‘©ä»ª", "terms": ["ç­‹è†œæª", "massage gun"]},
    ]
    
    SEARCH_MODIFIERS = [
        {"type": "æŠ€æœ¯åˆ›æ–°", "terms": ["æŠ€æœ¯", "æ–°å“", "ä¸“åˆ©", "new technology", "innovation", "patent", "launch"]},
        {"type": "å¸‚åœºè¯„æµ‹", "terms": ["è¯„æµ‹", "å¯¹æ¯”", "review", "vs"]},
        {"type": "æ³•è§„ç›‘ç®¡", "terms": ["ç›‘ç®¡", "æ³•è§„", "FDA approval", "clinical trial"]},
        {"type": "è¡Œä¸šæŠ¥å‘Š", "terms": ["è¡Œä¸šæŠ¥å‘Š", "å¸‚åœºè¶‹åŠ¿", "market research", "industry report"]},
    ]
    
    TARGETED_SOURCES = [
        {"domain": "36kr.com", "keywords": ["ç¾å®¹ç§‘æŠ€", "ä¸ªæŠ¤å®¶ç”µ", "æ¶ˆè´¹ç”µå­"]},
        {"domain": "geekpark.net", "keywords": ["æ™ºèƒ½ç¡¬ä»¶", "æ¶ˆè´¹ç”µå­"]},
        {"domain": "techcrunch.com", "keywords": ["beauty tech", "personal care", "hardware"]},
        {"domain": "theverge.com", "keywords": ["personal care", "gadgets", "review"]},
    ]
    
    DOMAIN_BLACKLIST = {
        'winbo4x4.com', 'youtube.com', 'bilibili.com', 'facebook.com',
        'twitter.com', 'instagram.com', 'pinterest.com', 'linkedin.com',
        'reddit.com', 'tiktok.com'
    }
    
    @classmethod
    def generate_search_tasks(cls) -> List[SearchTask]:
        """ç”Ÿæˆæœç´¢ä»»åŠ¡åˆ—è¡¨"""
        tasks = []
        
        # ç»„åˆæœç´¢è¯å’Œä¿®é¥°è¯
        for target in cls.SEARCH_TARGETS:
            for term in target["terms"]:
                for modifier in cls.SEARCH_MODIFIERS:
                    for mod_term in modifier["terms"]:
                        query = f'intitle:"{term}" "{mod_term}"'
                        tasks.append(SearchTask(
                            query=query,
                            sub_category=target["category"],
                            type=modifier["type"]
                        ))
        
        # é’ˆå¯¹ç‰¹å®šç½‘ç«™çš„æœç´¢
        for source in cls.TARGETED_SOURCES:
            for keyword in source["keywords"]:
                query = f'"{keyword}" site:{source["domain"]}'
                tasks.append(SearchTask(
                    query=query,
                    sub_category="è¡Œä¸šåŠ¨æ€",
                    type=source["domain"]
                ))
        
        logger.info(f"ç”Ÿæˆäº† {len(tasks)} ä¸ªæœç´¢ä»»åŠ¡")
        return tasks
    
    @classmethod
    def is_url_blacklisted(cls, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦åœ¨é»‘åå•ä¸­"""
        domain = urlparse(url).netloc
        return domain in cls.DOMAIN_BLACKLIST

# --- 5. APIå®¢æˆ·ç«¯ ---
class GoogleSearchClient:
    """Googleæœç´¢APIå®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str, search_engine_id: str):
        self.api_key = api_key
        self.search_engine_id = search_engine_id
        self.base_url = "https://www.googleapis.com/customsearch/v1"
    
    def search(self, query: str, num: int = 5) -> List[Dict]:
        """æ‰§è¡ŒGoogleæœç´¢"""
        params = {
            'key': self.api_key,
            'cx': self.search_engine_id,
            'q': query,
            'num': num
        }
        
        try:
            response = requests.get(self.base_url, params=params, timeout=15)
            response.raise_for_status()
            return response.json().get('items', [])
        except requests.RequestException as e:
            logger.error(f"Googleæœç´¢å¤±è´¥: {e}")
            return []

class DeepSeekClient:
    """DeepSeek APIå®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.deepseek.com/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    def _make_request(self, prompt: str, temperature: float = 0.1) -> Optional[str]:
        """å‘é€è¯·æ±‚åˆ°DeepSeek API"""
        try:
            response = requests.post(
                self.base_url,
                headers=self.headers,
                json={
                    "model": "deepseek-chat",
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": temperature,
                    "stream": False
                },
                timeout=30
            )
            response.raise_for_status()
            return response.json()['choices'][0]['message']['content']
        except requests.RequestException as e:
            logger.error(f"DeepSeek APIè¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def analyze_article(self, title: str, text: str) -> Optional[Dict]:
        """åˆ†ææ–‡ç« å†…å®¹"""
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡Œä¸šåˆ†æå¸ˆï¼Œä¸“æ³¨äºä¸ªæŠ¤å°å®¶ç”µé¢†åŸŸã€‚è¯·åˆ†æä»¥ä¸‹æ–‡ç« ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚

        æ–‡ç« æ ‡é¢˜: {title}
        æ–‡ç« å…¨æ–‡ï¼ˆéƒ¨åˆ†ï¼‰:
        {text[:3000]}

        è¯·æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
        1. **category**: åˆ¤æ–­æ–‡ç« çš„æ ¸å¿ƒç±»åˆ«ã€‚åªèƒ½ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªï¼š["æŠ€æœ¯åˆ›æ–°", "å¸‚åœºåŠ¨æ€", "æ³•è§„æ”¿ç­–", "ç«å“åˆ†æ", "ç”¨æˆ·åé¦ˆ", "è¡Œä¸šæŠ¥å‘Š", "æ— å…³"]ã€‚
        2. **summary**: ç”Ÿæˆä¸€æ®µä¸è¶…è¿‡200å­—çš„ä¸­æ–‡æ‘˜è¦ï¼Œç²¾å‡†æ¦‚æ‹¬æ–‡ç« æ ¸å¿ƒå†…å®¹ã€‚
        3. **keywords**: æå–3-5ä¸ªæœ€å…³é”®çš„ä¸­æ–‡å…³é”®è¯ã€‚

        è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
        {{
          "category": "...",
          "summary": "...",
          "keywords": ["...", "...", "..."]
        }}
        """
        
        content = self._make_request(prompt)
        if content:
            try:
                return json.loads(content)
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æå¤±è´¥: {e}")
                return None
        return None
    
    def evaluate_value(self, summary: str, sub_category: str) -> Optional[Dict]:
        """è¯„ä¼°æ–‡ç« ä¸šåŠ¡ä»·å€¼"""
        prompt = f"""
        ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ä¸ªæŠ¤å°å®¶ç”µäº§å“çº¿è´Ÿè´£äººã€‚è¯·åŸºäºä»¥ä¸‹æƒ…æŠ¥æ‘˜è¦ï¼Œè¯„ä¼°å…¶å¯¹äºæˆ‘ä»¬ä¸šåŠ¡çš„ä»·å€¼ã€‚

        æƒ…æŠ¥å­åˆ†ç±»: {sub_category}
        æƒ…æŠ¥æ‘˜è¦: {summary}

        è¯·æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
        1. **score**: å¯¹æƒ…æŠ¥çš„ä¸šåŠ¡ä»·å€¼æ‰“åˆ†ï¼ŒèŒƒå›´0-100ã€‚
        2. **reason**: ç”¨ä¸€å¥è¯ç®€æ˜æ‰¼è¦åœ°è§£é‡Šæ‰“åˆ†åŸå› ã€‚

        è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONï¼š
        {{
            "score": ...,
            "reason": "..."
        }}
        """
        
        content = self._make_request(prompt)
        if content:
            try:
                return json.loads(content)
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æå¤±è´¥: {e}")
                return None
        return None

# --- 6. æ–‡ç« å¤„ç†å™¨ ---
class ArticleProcessor:
    """æ–‡ç« å¤„ç†å™¨"""
    
    def __init__(self, config: AppConfig, deepseek_client: DeepSeekClient):
        self.config = config
        self.deepseek_client = deepseek_client
    
    def fetch_article(self, url: str) -> Optional[Article]:
        """æŠ“å–æ–‡ç« å†…å®¹"""
        try:
            article = Article(url, config=self.config.newspaper_config)
            article.download()
            article.parse()
            return article
        except Exception as e:
            logger.error(f"æ–‡ç« æŠ“å–å¤±è´¥ {url}: {e}")
            return None
    
    def extract_metadata(self, item: Dict) -> tuple:
        """ä»æœç´¢ç»“æœä¸­æå–å…ƒæ•°æ®"""
        pagemap = item.get('pagemap', {})
        metatags = pagemap.get('metatags', [{}])[0]
        publish_date = metatags.get('article:published_time', 'æœªçŸ¥')
        author = metatags.get('author', 'æœªçŸ¥')
        return publish_date, author
    
    def process_single_article(self, search_result: Dict, task: SearchTask) -> Optional[NewsArticle]:
        """å¤„ç†å•ç¯‡æ–‡ç« """
        url = search_result.get('link')
        if not url or SearchStrategyManager.is_url_blacklisted(url):
            return None
        
        # æå–åŸºæœ¬ä¿¡æ¯
        title = search_result.get('title', '')
        display_link = search_result.get('displayLink', '')
        publish_date, author = self.extract_metadata(search_result)
        
        # æŠ“å–æ–‡ç« å†…å®¹
        article = self.fetch_article(url)
        if not article or not article.text or len(article.text) < 200:
            logger.debug(f"æ–‡ç« å†…å®¹è¿‡çŸ­æˆ–æ— æ³•è·å–: {url}")
            return None
        
        # å¤„ç†å‘å¸ƒæ—¥æœŸ
        if publish_date and publish_date != "æœªçŸ¥":
            publish_date = publish_date.split('T')[0]
        elif article.publish_date:
            publish_date = article.publish_date.strftime('%Y-%m-%d')
        else:
            publish_date = "æœªçŸ¥"
        
        # AIåˆ†æ
        analysis = self.deepseek_client.analyze_article(title, article.text)
        if not analysis or analysis.get('category') == 'æ— å…³':
            logger.debug(f"æ–‡ç« è¢«AIè¿‡æ»¤: {title[:50]}...")
            return None
        
        # è¯„ä¼°ä»·å€¼
        summary = analysis.get('summary', '')
        evaluation = self.deepseek_client.evaluate_value(summary, task.sub_category)
        
        if not evaluation:
            value_score, value_reason = 0, 'è¯„ä¼°å¤±è´¥'
        else:
            value_score = evaluation.get('score', 0)
            value_reason = evaluation.get('reason', 'è¯„ä¼°å¤±è´¥')
        
        return NewsArticle(
            title=title,
            url=url,
            source=display_link,
            publish_date=publish_date,
            author=author,
            sub_category=task.sub_category,
            category=analysis.get('category', 'å…¶ä»–'),
            summary=summary,
            keywords=", ".join(analysis.get('keywords', [])),
            value_score=value_score,
            value_reason=value_reason
        )

# --- 7. ä¸»ç¨‹åº ---
class NewsScraper:
    """æ–°é—»æŠ“å–ä¸»ç¨‹åº"""
    
    def __init__(self):
        self.config = AppConfig()
        self.db_manager = DatabaseManager(self.config.DB_FILE)
        self.google_client = GoogleSearchClient(
            self.config.GOOGLE_API_KEY,
            self.config.SEARCH_ENGINE_ID
        )
        self.deepseek_client = DeepSeekClient(self.config.DEEPSEEK_API_KEY)
        self.article_processor = ArticleProcessor(self.config, self.deepseek_client)
        self.existing_urls = self.db_manager.get_existing_urls()
    
    def process_search_task(self, task: SearchTask) -> List[NewsArticle]:
        """å¤„ç†å•ä¸ªæœç´¢ä»»åŠ¡"""
        logger.info(f"æ‰§è¡Œæœç´¢: {task.query}")
        search_results = self.google_client.search(task.query, num=5)
        
        if not search_results:
            return []
        
        articles = []
        for result in search_results:
            url = result.get('link')
            if url and url not in self.existing_urls:
                article = self.article_processor.process_single_article(result, task)
                if article:
                    articles.append(article)
                    self.existing_urls.add(url)
                    logger.info(f"æˆåŠŸå¤„ç†: {article.title[:50]}... (ä»·å€¼åˆ†: {article.value_score})")
        
        return articles
    
    def run(self):
        """ä¸»æ‰§è¡Œå‡½æ•°"""
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæƒ…æŠ¥æŠ“å–ä»»åŠ¡...")
        logger.info(f"ğŸ“Š æ•°æ®åº“ä¸­å·²å­˜åœ¨ {len(self.existing_urls)} æ¡è®°å½•")
        
        # ç”Ÿæˆæœç´¢ä»»åŠ¡
        search_tasks = SearchStrategyManager.generate_search_tasks()
        all_articles = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.config.MAX_WORKERS) as executor:
            future_to_task = {
                executor.submit(self.process_search_task, task): task
                for task in search_tasks
            }
            
            for future in tqdm(as_completed(future_to_task), total=len(search_tasks), desc="å¤„ç†æœç´¢ä»»åŠ¡"):
                try:
                    articles = future.result()
                    all_articles.extend(articles)
                    time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                except Exception as e:
                    logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        
        # ä¿å­˜ç»“æœ
        if all_articles:
            saved_count = self.db_manager.save_articles(all_articles)
            logger.info(f"âœ… æˆåŠŸä¿å­˜ {saved_count} ç¯‡æ–°æ–‡ç« ")
            
            # æ˜¾ç¤ºé«˜ä»·å€¼æ–‡ç« 
            high_value_articles = [a for a in all_articles if a.value_score >= 70]
            if high_value_articles:
                logger.info("\nğŸŒŸ é«˜ä»·å€¼æƒ…æŠ¥ (â‰¥70åˆ†):")
                for article in sorted(high_value_articles, key=lambda x: x.value_score, reverse=True):
                    logger.info(f"  [{article.value_score}åˆ†] {article.title[:60]}...")
                    logger.info(f"    ç†ç”±: {article.value_reason}")
        else:
            logger.info("æœ¬æ¬¡è¿è¡Œæœªå‘ç°æ–°æƒ…æŠ¥")

# --- 8. ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    scraper = NewsScraper()
    scraper.run()
